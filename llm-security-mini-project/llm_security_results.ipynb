{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c6eb8b2d",
      "metadata": {},
      "source": [
        "# LLM Security Results Viewer\n",
        "\n",
        "This notebook loads the latest evaluation summaries from `outputs/` and shows tables and plots.\n",
        "Run cells from top to bottom. It does **not** run training; it only visualizes results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20e123a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "from IPython.display import display\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "except Exception:\n",
        "    pd = None\n",
        "\n",
        "base = Path(\".\")\n",
        "outputs = base / \"outputs\"\n",
        "summary_csv = outputs / \"summary_table.csv\"\n",
        "\n",
        "if not outputs.exists():\n",
        "    print(\"No outputs directory found. Run evaluations first.\")\n",
        "else:\n",
        "    rows = []\n",
        "    if summary_csv.exists() and pd is not None:\n",
        "        df = pd.read_csv(summary_csv)\n",
        "    else:\n",
        "        for path in sorted(outputs.glob(\"summary_*.json\")):\n",
        "            with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "            tag = data.get(\"experiment_tag\") or path.stem.replace(\"summary_\", \"\")\n",
        "            defense = data.get(\"defense\", \"\")\n",
        "            config = data.get(\"config\", \"\")\n",
        "            for model_name, stats in data.get(\"models\", {}).items():\n",
        "                inj = stats.get(\"injection\", {})\n",
        "                ben = stats.get(\"benign\", {})\n",
        "                overall = stats.get(\"overall\", {})\n",
        "                rows.append(\n",
        "                    {\n",
        "                        \"experiment_tag\": tag,\n",
        "                        \"summary_file\": str(path),\n",
        "                        \"model\": model_name,\n",
        "                        \"defense\": defense,\n",
        "                        \"config\": config,\n",
        "                        \"injection_success_rate\": inj.get(\"attack_success_rate\", 0.0),\n",
        "                        \"benign_refusal_rate\": ben.get(\"refusal_rate\", 0.0),\n",
        "                        \"avg_latency_s\": overall.get(\"avg_latency_s\", 0.0),\n",
        "                    }\n",
        "                )\n",
        "        df = pd.DataFrame(rows) if pd is not None else rows\n",
        "\n",
        "    display(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de0e4ecf",
      "metadata": {},
      "outputs": [],
      "source": [
        "if pd is None:\n",
        "    print(\"pandas not available; table displayed as raw rows above.\")\n",
        "else:\n",
        "    cols = [\n",
        "        \"experiment_tag\",\n",
        "        \"model\",\n",
        "        \"defense\",\n",
        "        \"injection_success_rate\",\n",
        "        \"benign_refusal_rate\",\n",
        "        \"avg_latency_s\",\n",
        "    ]\n",
        "    display(df[cols].sort_values([\"experiment_tag\", \"model\"]))\n",
        "\n",
        "    display(\n",
        "        df.groupby([\"experiment_tag\", \"defense\"])[\n",
        "            [\"injection_success_rate\", \"benign_refusal_rate\", \"avg_latency_s\"]\n",
        "        ]\n",
        "        .mean()\n",
        "        .sort_values(\"injection_success_rate\", ascending=False)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b271d33",
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, Markdown\n",
        "\n",
        "plot_dir = outputs / \"plots\"\n",
        "if not plot_dir.exists():\n",
        "    print(\"No plots found. Run: python scripts/plot_results.py\")\n",
        "else:\n",
        "    for img in sorted(plot_dir.glob(\"*.png\")):\n",
        "        display(Markdown(f\"### {img.name}\"))\n",
        "        display(Image(filename=str(img)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6ba5adf",
      "metadata": {},
      "source": [
        "## Run new experiments (GPU)\n",
        "\n",
        "Use Slurm on Triton (V100 16G):\n",
        "\n",
        "```\n",
        "sbatch --export=ALL,EVAL_CONFIG=configs/eval_small_phi3.yaml,DEFENSE=none scripts/run_eval_v100.sh\n",
        "sbatch --export=ALL,EVAL_CONFIG=configs/eval_small_phi3.yaml,DEFENSE=filter_prefix scripts/run_eval_v100.sh\n",
        "```\n",
        "\n",
        "Then refresh plots:\n",
        "\n",
        "```\n",
        "python scripts/plot_results.py\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

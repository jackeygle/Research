{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":121150,"databundleVersionId":15061024,"sourceType":"competition"},{"sourceId":14339497,"sourceType":"datasetVersion","datasetId":9155323},{"sourceId":14374989,"sourceType":"datasetVersion","datasetId":9147887},{"sourceId":14376272,"sourceType":"datasetVersion","datasetId":9181082},{"sourceId":14410665,"sourceType":"datasetVersion","datasetId":9203761},{"sourceId":14624744,"sourceType":"datasetVersion","datasetId":9341680},{"sourceId":14661932,"sourceType":"datasetVersion","datasetId":9366631}],"dockerImageVersionId":31236,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nCFG = {\n    \"data_path\": \"/kaggle/input/deep-past-initiative-machine-translation/test.csv\",\n    \"models\": [\n        \"/kaggle/input/byt5-base-big-data2\",\n        \"/kaggle/input/byt5-akkadian-model\",\n        \"/kaggle/input/d/assiaben/final-byt5/byt5-akkadian-optimized-34x\"\n    ],\n    \"weights\": [0.995, 0.985, 0.99], \n    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    \"max_len\": 512,\n    \"batch_size\": 8,\n    \"gen_params\": {\n        \"num_beams\": 8,\n        \"max_new_tokens\": 512,\n        \"length_penalty\": 1.10,\n        \"early_stopping\": True\n    }\n}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-30T07:14:11.05102Z","iopub.execute_input":"2026-01-30T07:14:11.051252Z","iopub.status.idle":"2026-01-30T07:14:23.048511Z","shell.execute_reply.started":"2026-01-30T07:14:11.051219Z","shell.execute_reply":"2026-01-30T07:14:23.047902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_input(text):\n    if pd.isna(text): return \"\"\n    t = str(text)\n    t = re.sub(r'(\\.{3,}|…+|……)', '<big_gap>', t)\n    t = re.sub(r'(xx+|\\s+x\\s+)', '<gap>', t)\n    return t\n\ndef hnr(text):\n    if not isinstance(text, str) or not text.strip(): \n        return \"\"\n    \n    t = text\n    t = t.replace('ḫ', 'h').replace('Ḫ', 'H')\n    sub_map = str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\")\n    t = t.translate(sub_map)\n    t = re.sub(r'(\\[x\\]|\\(x\\)|\\bx\\b)', '<gap>', t, flags=re.I)\n    t = re.sub(r'(\\.{3,}|…|\\[\\.+\\])', '<big_gap>', t)\n    t = re.sub(r'<gap>\\s*<gap>', ' <big_gap> ', t)\n    t = re.sub(r'<big_gap>\\s*<big_gap>', ' <big_gap> ', t)\n    t = re.sub(\n        r'\\((fem|plur|pl|sing|singular|plural|\\?|!)\\.?\\s*\\w*\\)', \n        '', \n        t, \n        flags=re.I\n    )\n    t = t.replace('<gap>', '\\x00GAP\\x00').replace('<big_gap>', '\\x00BIG\\x00')\n    bad_chars = '!?()\"—–<>⌈⌋⌊[]+ʾ/;'\n    t = t.translate(str.maketrans('', '', bad_chars))\n    t = t.replace('\\x00GAP\\x00', ' <gap> ').replace('\\x00BIG\\x00', ' <big_gap> ')\n    t = t.replace(\"ד\", \"\")\n    frac_map = {\n        r'\\.5\\b': ' ½', r'\\.25\\b': ' ¼', r'\\.75\\b': ' ¾',\n        r'\\.33+\\d*\\b': ' ⅓', r'\\.66+\\d*\\b': ' ⅔'\n    }\n    for pat, rep in frac_map.items():\n        t = re.sub(r'(\\d+)' + pat, r'\\1' + rep, t)\n        t = re.sub(r'\\b0' + pat, rep.strip(), t)\n    t = re.sub(r'\\b(\\w+)(?:\\s+\\1\\b)+', r'\\1', t)\n    for n in range(4, 1, -1):\n        pat = r'\\b((?:\\w+\\s+){' + str(n-1) + r'}\\w+)(?:\\s+\\1\\b)+'\n        t = re.sub(pat, r'\\1', t)\n    t = re.sub(r'\\s+([.,:])', r'\\1', t)\n    t = re.sub(r'([.,])\\1+', r'\\1', t)\n    return re.sub(r'\\s+', ' ', t).strip().strip('-').strip()\n\ndef load_blended_model():\n    total_score = sum(CFG['weights'])\n    W = [w / total_score for w in CFG['weights']]\n    \n    base_model = AutoModelForSeq2SeqLM.from_pretrained(CFG['models'][1])\n    final_sd = base_model.state_dict()\n    \n    sd_m1 = AutoModelForSeq2SeqLM.from_pretrained(CFG['models'][0]).state_dict()\n    sd_m3 = AutoModelForSeq2SeqLM.from_pretrained(CFG['models'][2]).state_dict()\n    \n    for k in final_sd:\n        val = W[1] * final_sd[k]\n        norm = W[1]\n        \n        if k in sd_m1: \n            val += W[0] * sd_m1[k]\n            norm += W[0]\n        if k in sd_m3: \n            val += W[2] * sd_m3[k]\n            norm += W[2]\n            \n        final_sd[k] = val / norm\n        \n    base_model.load_state_dict(final_sd)\n    return base_model.to(CFG['device']).eval().float()\n\nclass AkkadDataset(Dataset):\n    def __init__(self, df):\n        self.ids = df['id'].tolist()\n        self.texts = [\"translate Akkadian to English: \" + str(t) for t in df['transliteration']]\n    def __len__(self): return len(self.ids)\n    def __getitem__(self, i): return self.ids[i], self.texts[i]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T07:14:23.049876Z","iopub.execute_input":"2026-01-30T07:14:23.050314Z","iopub.status.idle":"2026-01-30T07:14:23.062353Z","shell.execute_reply.started":"2026-01-30T07:14:23.050288Z","shell.execute_reply":"2026-01-30T07:14:23.06163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    df = pd.read_csv(CFG['data_path'])\n    df['transliteration'] = df['transliteration'].apply(clean_input)\n    \n    model = load_blended_model()\n    tokenizer = AutoTokenizer.from_pretrained(CFG['models'][1])\n    \n    loader = DataLoader(\n        AkkadDataset(df), \n        batch_size=CFG['batch_size'], \n        shuffle=False,\n        num_workers=2,\n        collate_fn=lambda b: (\n            [x[0] for x in b], \n            tokenizer([x[1] for x in b], max_length=CFG['max_len'], padding=True, truncation=True, return_tensors=\"pt\")\n        )\n    )\n    \n    results = []\n    \n    with torch.inference_mode():\n        for ids, inputs in loader:\n            outputs = model.generate(\n                input_ids=inputs.input_ids.to(CFG['device']),\n                attention_mask=inputs.attention_mask.to(CFG['device']),\n                **CFG['gen_params']\n            )\n            \n            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n            cleaned = [hnr(txt) for txt in decoded]\n            \n            results.extend(zip(ids, cleaned))\n\n    sub = pd.DataFrame(results, columns=['id', 'translation'])\n    sub.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T07:14:23.06349Z","iopub.execute_input":"2026-01-30T07:14:23.063794Z","iopub.status.idle":"2026-01-30T07:15:24.113767Z","shell.execute_reply.started":"2026-01-30T07:14:23.06376Z","shell.execute_reply":"2026-01-30T07:15:24.113031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for index, row in sub.iterrows():\n    print(f\"ID: {row['id']}\")\n    print(f\"Translation: {row['translation']}\")\n    print(\"-\" * 20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T07:15:24.114893Z","iopub.execute_input":"2026-01-30T07:15:24.115223Z","iopub.status.idle":"2026-01-30T07:15:24.120895Z","shell.execute_reply.started":"2026-01-30T07:15:24.115161Z","shell.execute_reply":"2026-01-30T07:15:24.120266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Deep Past - Multi-Architecture Ensemble Inference\n",
                "\n",
                "Uses **Output Blending** (voting) to combine predictions from different model architectures."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, re, gc\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "from pathlib import Path\n",
                "from collections import Counter\n",
                "from tqdm import tqdm\n",
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
                "\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model paths - update these with your trained model paths\n",
                "MODEL_PATHS = [\n",
                "    '/kaggle/input/deep-past-models-1/M1_byt5_baseline',\n",
                "    '/kaggle/input/deep-past-models-1/M2_byt5_bidir',\n",
                "    '/kaggle/input/deep-past-models-1/M3_byt5_long',\n",
                "    '/kaggle/input/deep-past-models-2/M4_byt5_full',\n",
                "    '/kaggle/input/deep-past-models-2/M5_mt5_baseline',\n",
                "    '/kaggle/input/deep-past-models-2/M6_mt5_bidir',\n",
                "    '/kaggle/input/deep-past-models-3/M7_t5_small',\n",
                "    '/kaggle/input/deep-past-models-3/M8_nllb',\n",
                "    '/kaggle/input/deep-past-models-3/M9_nllb_bidir',\n",
                "]\n",
                "\n",
                "# Check available models\n",
                "available = [p for p in MODEL_PATHS if os.path.exists(p)]\n",
                "print(f\"Found {len(available)} models\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load test data\n",
                "test_df = pd.read_csv('/kaggle/input/deep-past-initiative-machine-translation/test.csv')\n",
                "print(f\"Test samples: {len(test_df)}\")\n",
                "test_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== Inference Functions ==========\n",
                "def translate_with_model(model_path, texts, num_beams=8, max_new_tokens=512):\n",
                "    \"\"\"Generate translations using a single model.\"\"\"\n",
                "    print(f\"Loading: {Path(model_path).name}\")\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
                "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEVICE).eval()\n",
                "    \n",
                "    translations = []\n",
                "    for text in tqdm(texts, desc=\"Translating\"):\n",
                "        input_text = f\"translate Akkadian to English: {text}\"\n",
                "        inputs = tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True).to(DEVICE)\n",
                "        with torch.no_grad():\n",
                "            outputs = model.generate(**inputs, num_beams=num_beams, max_new_tokens=max_new_tokens, early_stopping=True)\n",
                "        translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "        translations.append(translation)\n",
                "    \n",
                "    del model; gc.collect(); torch.cuda.empty_cache()\n",
                "    return translations\n",
                "\n",
                "def majority_vote(all_translations):\n",
                "    \"\"\"Select most common translation from ensemble.\"\"\"\n",
                "    results = []\n",
                "    for i in range(len(all_translations[0])):\n",
                "        candidates = [t[i] for t in all_translations]\n",
                "        # Use most common translation\n",
                "        counter = Counter(candidates)\n",
                "        best = counter.most_common(1)[0][0]\n",
                "        results.append(best)\n",
                "    return results\n",
                "\n",
                "def longest_common_vote(all_translations):\n",
                "    \"\"\"Select translation closest to average length.\"\"\"\n",
                "    results = []\n",
                "    for i in range(len(all_translations[0])):\n",
                "        candidates = [t[i] for t in all_translations]\n",
                "        avg_len = np.mean([len(c) for c in candidates])\n",
                "        # Select candidate closest to average length\n",
                "        best = min(candidates, key=lambda x: abs(len(x) - avg_len))\n",
                "        results.append(best)\n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== Run Ensemble Inference ==========\n",
                "texts = test_df['transliteration'].tolist()\n",
                "all_translations = []\n",
                "\n",
                "for model_path in available:\n",
                "    translations = translate_with_model(model_path, texts)\n",
                "    all_translations.append(translations)\n",
                "    print(f\"Completed: {Path(model_path).name}\")\n",
                "\n",
                "print(f\"\\nTotal models: {len(all_translations)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== Ensemble Voting ==========\n",
                "final_translations = majority_vote(all_translations)\n",
                "\n",
                "# Show comparison\n",
                "for i in range(min(2, len(texts))):\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Input: {texts[i][:100]}...\")\n",
                "    for j, trans in enumerate(all_translations):\n",
                "        print(f\"M{j+1}: {trans[i][:80]}...\")\n",
                "    print(f\"FINAL: {final_translations[i][:80]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== Postprocessing ==========\n",
                "def postprocess(text):\n",
                "    if not isinstance(text, str): return \"\"\n",
                "    text = text.translate(str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\"))\n",
                "    text = text.replace('ḫ','h').replace('Ḫ','H')\n",
                "    for old, new in {'–':'-', '—':'-', '"':'\"', '"':'\"', \"'\":\"'\", \"'\":\"'\"}.items():\n",
                "        text = text.replace(old, new)\n",
                "    text = re.sub(r'\\b(\\w+)(?:\\s+\\1\\b)+', r'\\1', text)\n",
                "    text = re.sub(r'\\s+([.,:;!?])', r'\\1', text)\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    if text and text[0].isalpha():\n",
                "        text = text[0].upper() + text[1:]\n",
                "    return text\n",
                "\n",
                "cleaned = [postprocess(t) for t in final_translations]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== Create Submission ==========\n",
                "submission = pd.DataFrame({\n",
                "    'id': test_df['id'],\n",
                "    'translation': cleaned\n",
                "})\n",
                "submission.to_csv('submission.csv', index=False)\n",
                "print(\"Submission saved!\")\n",
                "submission"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
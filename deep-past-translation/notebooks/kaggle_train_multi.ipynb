{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Deep Past - Multi-Architecture 9-Model Ensemble\n",
                "\n",
                "**Models**: ByT5-small (4) + mT5-small (2) + T5-small (1) + NLLB-200 (2)\n",
                "\n",
                "Run with `MODEL_GROUP = 1, 2, or 3` to train different sets (~3h each)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== Configuration ==========\n",
                "MODEL_GROUP = 1  # 1: M1-M3 (ByT5), 2: M4-M6 (ByT5+mT5), 3: M7-M9 (T5+NLLB)\n",
                "\n",
                "import os, gc, re, unicodedata\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "from pathlib import Path\n",
                "from datasets import Dataset\n",
                "from transformers import (\n",
                "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
                "    DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
                ")\n",
                "import evaluate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Multi-architecture model configurations\n",
                "CONFIGS = {\n",
                "    # ByT5-small variants (4 models, ~4h total)\n",
                "    1: {'name': 'M1_byt5_baseline', 'model': 'google/byt5-small', 'epochs': 20, 'lr': 1e-4, 'bs': 1, 'ga': 8, 'bidir': False, 'max_len': 512},\n",
                "    2: {'name': 'M2_byt5_bidir', 'model': 'google/byt5-small', 'epochs': 20, 'lr': 1e-4, 'bs': 1, 'ga': 8, 'bidir': True, 'max_len': 512},\n",
                "    3: {'name': 'M3_byt5_long', 'model': 'google/byt5-small', 'epochs': 30, 'lr': 1e-4, 'bs': 1, 'ga': 8, 'bidir': False, 'max_len': 512},\n",
                "    4: {'name': 'M4_byt5_full', 'model': 'google/byt5-small', 'epochs': 20, 'lr': 1e-4, 'bs': 1, 'ga': 8, 'bidir': True, 'max_len': 512},\n",
                "    \n",
                "    # mT5-small variants (2 models, ~1.5h total)\n",
                "    5: {'name': 'M5_mt5_baseline', 'model': 'google/mt5-small', 'epochs': 20, 'lr': 1e-4, 'bs': 2, 'ga': 4, 'bidir': False, 'max_len': 512},\n",
                "    6: {'name': 'M6_mt5_bidir', 'model': 'google/mt5-small', 'epochs': 20, 'lr': 1e-4, 'bs': 2, 'ga': 4, 'bidir': True, 'max_len': 512},\n",
                "    \n",
                "    # T5-small (1 model, ~0.5h)\n",
                "    7: {'name': 'M7_t5_small', 'model': 'google/t5-small', 'epochs': 25, 'lr': 1e-4, 'bs': 4, 'ga': 2, 'bidir': False, 'max_len': 512},\n",
                "    \n",
                "    # NLLB-200 (2 models, ~3h total)\n",
                "    8: {'name': 'M8_nllb', 'model': 'facebook/nllb-200-distilled-600M', 'epochs': 15, 'lr': 5e-5, 'bs': 1, 'ga': 8, 'bidir': False, 'max_len': 256},\n",
                "    9: {'name': 'M9_nllb_bidir', 'model': 'facebook/nllb-200-distilled-600M', 'epochs': 15, 'lr': 5e-5, 'bs': 1, 'ga': 8, 'bidir': True, 'max_len': 256},\n",
                "}\n",
                "\n",
                "# Select models for this run\n",
                "GROUP_MAP = {1: [1, 2, 3], 2: [4, 5, 6], 3: [7, 8, 9]}\n",
                "MODEL_IDS = GROUP_MAP[MODEL_GROUP]\n",
                "print(f\"Training: {[CONFIGS[i]['name'] for i in MODEL_IDS]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== Data Preprocessing ==========\n",
                "def normalize_text(text):\n",
                "    if pd.isna(text): return \"\"\n",
                "    text = str(text).strip()\n",
                "    text = text.translate(str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\"))\n",
                "    text = unicodedata.normalize('NFKD', text)\n",
                "    text = re.sub(r'\\.{3,}|…+', ' <big_gap> ', text)\n",
                "    text = re.sub(r'xx+|\\s+x\\s+', ' <gap> ', text, flags=re.I)\n",
                "    text = re.sub(r'[\\[\\]<>⌈⌋⌊]', '', text)\n",
                "    return re.sub(r'\\s+', ' ', text).strip()\n",
                "\n",
                "def sentence_align(df):\n",
                "    aligned = []\n",
                "    for _, row in df.iterrows():\n",
                "        src, tgt = str(row.get('transliteration', '')), str(row.get('translation', ''))\n",
                "        tgt_sents = [t.strip() for t in re.split(r'(?<=[.!?])\\s+', tgt) if t.strip()]\n",
                "        src_lines = [s.strip() for s in src.split('\\n') if s.strip()]\n",
                "        if len(tgt_sents) > 1 and len(tgt_sents) == len(src_lines):\n",
                "            for s, t in zip(src_lines, tgt_sents):\n",
                "                if len(s) > 3 and len(t) > 3:\n",
                "                    aligned.append({'transliteration': normalize_text(s), 'translation': t.strip()})\n",
                "        else:\n",
                "            aligned.append({'transliteration': normalize_text(src), 'translation': tgt.strip()})\n",
                "    return pd.DataFrame(aligned)\n",
                "\n",
                "def create_bidirectional(df):\n",
                "    fwd = df.copy()\n",
                "    fwd['input_text'] = \"translate Akkadian to English: \" + fwd['transliteration'].astype(str)\n",
                "    fwd['target_text'] = fwd['translation'].astype(str)\n",
                "    bwd = df.copy()\n",
                "    bwd['input_text'] = \"translate English to Akkadian: \" + bwd['translation'].astype(str)\n",
                "    bwd['target_text'] = bwd['transliteration'].astype(str)\n",
                "    return pd.concat([fwd, bwd], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
                "\n",
                "def load_data(use_bidir=False):\n",
                "    data_path = Path('/kaggle/input/deep-past-initiative-machine-translation')\n",
                "    df = sentence_align(pd.read_csv(data_path / 'train.csv'))\n",
                "    print(f\"After alignment: {len(df)}\")\n",
                "    if use_bidir:\n",
                "        df = create_bidirectional(df)\n",
                "        print(f\"After bidirectional: {len(df)}\")\n",
                "    else:\n",
                "        df['input_text'] = \"translate Akkadian to English: \" + df['transliteration'].astype(str)\n",
                "        df['target_text'] = df['translation'].astype(str)\n",
                "    return df[['input_text', 'target_text']]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== Training Function ==========\n",
                "def train_model(cfg):\n",
                "    print(f\"\\n{'='*60}\\nTraining: {cfg['name']} ({cfg['model']})\\n{'='*60}\")\n",
                "    gc.collect(); torch.cuda.empty_cache()\n",
                "    \n",
                "    df = load_data(use_bidir=cfg['bidir'])\n",
                "    dataset = Dataset.from_pandas(df)\n",
                "    split = dataset.train_test_split(test_size=0.1, seed=42)\n",
                "    \n",
                "    tokenizer = AutoTokenizer.from_pretrained(cfg['model'])\n",
                "    model = AutoModelForSeq2SeqLM.from_pretrained(cfg['model'])\n",
                "    \n",
                "    def preprocess(examples):\n",
                "        inputs = [str(x) for x in examples['input_text']]\n",
                "        targets = [str(x) for x in examples['target_text']]\n",
                "        model_inputs = tokenizer(inputs, max_length=cfg['max_len'], truncation=True)\n",
                "        labels = tokenizer(targets, max_length=cfg['max_len'], truncation=True)\n",
                "        model_inputs['labels'] = labels['input_ids']\n",
                "        return model_inputs\n",
                "    \n",
                "    train_tok = split['train'].map(preprocess, batched=True, remove_columns=split['train'].column_names)\n",
                "    val_tok = split['test'].map(preprocess, batched=True, remove_columns=split['test'].column_names)\n",
                "    \n",
                "    metric_chrf = evaluate.load('chrf')\n",
                "    metric_bleu = evaluate.load('sacrebleu')\n",
                "    \n",
                "    def compute_metrics(eval_preds):\n",
                "        preds, labels = eval_preds\n",
                "        if isinstance(preds, tuple): preds = preds[0]\n",
                "        if hasattr(preds, 'ndim') and preds.ndim == 3: preds = np.argmax(preds, axis=-1)\n",
                "        preds = np.clip(preds.astype(np.int64), 0, tokenizer.vocab_size - 1)\n",
                "        dec_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
                "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
                "        dec_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
                "        chrf = metric_chrf.compute(predictions=dec_preds, references=dec_labels)['score']\n",
                "        bleu = metric_bleu.compute(predictions=dec_preds, references=[[x] for x in dec_labels])['score']\n",
                "        return {'chrf': chrf, 'bleu': bleu, 'geo_mean': (chrf*bleu)**0.5 if chrf>0 and bleu>0 else 0}\n",
                "    \n",
                "    output_dir = f\"./models/{cfg['name']}\"\n",
                "    args = Seq2SeqTrainingArguments(\n",
                "        output_dir=output_dir, eval_strategy='epoch', save_strategy='epoch',\n",
                "        learning_rate=cfg['lr'], optim='adafactor', label_smoothing_factor=0.2,\n",
                "        fp16=False, per_device_train_batch_size=cfg['bs'], per_device_eval_batch_size=cfg['bs'],\n",
                "        gradient_accumulation_steps=cfg['ga'], weight_decay=0.01, save_total_limit=1,\n",
                "        num_train_epochs=cfg['epochs'], predict_with_generate=True, logging_steps=50,\n",
                "        report_to='none', load_best_model_at_end=True, metric_for_best_model='geo_mean', greater_is_better=True,\n",
                "    )\n",
                "    \n",
                "    trainer = Seq2SeqTrainer(\n",
                "        model=model, args=args, train_dataset=train_tok, eval_dataset=val_tok,\n",
                "        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
                "        tokenizer=tokenizer, compute_metrics=compute_metrics,\n",
                "    )\n",
                "    trainer.train()\n",
                "    trainer.save_model(output_dir)\n",
                "    tokenizer.save_pretrained(output_dir)\n",
                "    print(f\"Saved: {output_dir}\")\n",
                "    del model, trainer; gc.collect(); torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== Train Models ==========\n",
                "for mid in MODEL_IDS:\n",
                "    train_model(CONFIGS[mid])\n",
                "\n",
                "print(\"\\n\" + \"=\"*60 + \"\\n✅ Training Complete!\\n\" + \"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!ls -la ./models/"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
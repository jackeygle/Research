{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Deep Past Challenge - ByT5 9-Model Ensemble Training\n",
                "\n",
                "This notebook trains 3 out of 9 ByT5-small models for the Akkadian-English translation task.\n",
                "Run this notebook 3 times with different `MODEL_GROUP` values (1, 2, 3) to train all 9 models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration - Change this for each run\n",
                "MODEL_GROUP = 1  # 1: M1-M3, 2: M4-M6, 3: M7-M9\n",
                "\n",
                "import os\n",
                "import gc\n",
                "import re\n",
                "import unicodedata\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "from pathlib import Path\n",
                "from datasets import Dataset\n",
                "from sklearn.model_selection import train_test_split\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForSeq2SeqLM,\n",
                "    DataCollatorForSeq2Seq,\n",
                "    Seq2SeqTrainingArguments,\n",
                "    Seq2SeqTrainer,\n",
                ")\n",
                "import evaluate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model configurations for all 9 models\n",
                "CONFIGS = {\n",
                "    1: {'name': 'M1_baseline', 'epochs': 20, 'lr': 1e-4, 'bidir': False, 'extra': False, 'label_smooth': 0.2, 'max_len': 512},\n",
                "    2: {'name': 'M2_bidirectional', 'epochs': 20, 'lr': 1e-4, 'bidir': True, 'extra': False, 'label_smooth': 0.2, 'max_len': 512},\n",
                "    3: {'name': 'M3_long_train', 'epochs': 30, 'lr': 1e-4, 'bidir': False, 'extra': False, 'label_smooth': 0.2, 'max_len': 512},\n",
                "    4: {'name': 'M4_low_lr', 'epochs': 20, 'lr': 5e-5, 'bidir': False, 'extra': False, 'label_smooth': 0.2, 'max_len': 512},\n",
                "    5: {'name': 'M5_high_lr', 'epochs': 15, 'lr': 2e-4, 'bidir': False, 'extra': False, 'label_smooth': 0.2, 'max_len': 512},\n",
                "    6: {'name': 'M6_extra_data', 'epochs': 20, 'lr': 1e-4, 'bidir': False, 'extra': True, 'label_smooth': 0.2, 'max_len': 512},\n",
                "    7: {'name': 'M7_no_smoothing', 'epochs': 20, 'lr': 1e-4, 'bidir': False, 'extra': False, 'label_smooth': 0.0, 'max_len': 512},\n",
                "    8: {'name': 'M8_short_len', 'epochs': 20, 'lr': 1e-4, 'bidir': False, 'extra': False, 'label_smooth': 0.2, 'max_len': 256},\n",
                "    9: {'name': 'M9_full_augment', 'epochs': 20, 'lr': 1e-4, 'bidir': True, 'extra': True, 'label_smooth': 0.2, 'max_len': 512},\n",
                "}\n",
                "\n",
                "# Select models for this run\n",
                "if MODEL_GROUP == 1:\n",
                "    MODEL_IDS = [1, 2, 3]\n",
                "elif MODEL_GROUP == 2:\n",
                "    MODEL_IDS = [4, 5, 6]\n",
                "else:\n",
                "    MODEL_IDS = [7, 8, 9]\n",
                "\n",
                "print(f\"Training models: {[CONFIGS[i]['name'] for i in MODEL_IDS]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== Data Preprocessing ==========\n",
                "def normalize_text(text):\n",
                "    if pd.isna(text) or not isinstance(text, str):\n",
                "        return \"\"\n",
                "    text = str(text).strip()\n",
                "    subscript_map = str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\")\n",
                "    text = text.translate(subscript_map)\n",
                "    text = unicodedata.normalize('NFKD', text)\n",
                "    text = re.sub(r'\\.{3,}|…+', ' <big_gap> ', text)\n",
                "    text = re.sub(r'xx+|\\s+x\\s+', ' <gap> ', text, flags=re.IGNORECASE)\n",
                "    text = re.sub(r'[\\[\\]<>⌈⌋⌊]', '', text)\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    return text\n",
                "\n",
                "def sentence_align(df):\n",
                "    aligned = []\n",
                "    for _, row in df.iterrows():\n",
                "        src = str(row.get('transliteration', ''))\n",
                "        tgt = str(row.get('translation', ''))\n",
                "        tgt_sents = [t.strip() for t in re.split(r'(?<=[.!?])\\s+', tgt) if t.strip()]\n",
                "        src_lines = [s.strip() for s in src.split('\\n') if s.strip()]\n",
                "        if len(tgt_sents) > 1 and len(tgt_sents) == len(src_lines):\n",
                "            for s, t in zip(src_lines, tgt_sents):\n",
                "                if len(s) > 3 and len(t) > 3:\n",
                "                    aligned.append({'transliteration': normalize_text(s), 'translation': t.strip()})\n",
                "        else:\n",
                "            aligned.append({'transliteration': normalize_text(src), 'translation': tgt.strip()})\n",
                "    return pd.DataFrame(aligned)\n",
                "\n",
                "def create_bidirectional(df):\n",
                "    fwd = df.copy()\n",
                "    fwd['input_text'] = \"translate Akkadian to English: \" + fwd['transliteration'].astype(str)\n",
                "    fwd['target_text'] = fwd['translation'].astype(str)\n",
                "    bwd = df.copy()\n",
                "    bwd['input_text'] = \"translate English to Akkadian: \" + bwd['translation'].astype(str)\n",
                "    bwd['target_text'] = bwd['transliteration'].astype(str)\n",
                "    return pd.concat([fwd, bwd], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
                "\n",
                "def load_data(use_bidir=False, use_extra=False):\n",
                "    data_path = Path('/kaggle/input/deep-past-initiative-machine-translation')\n",
                "    train_df = pd.read_csv(data_path / 'train.csv')\n",
                "    train_df = sentence_align(train_df)\n",
                "    print(f\"After alignment: {len(train_df)}\")\n",
                "    \n",
                "    if use_bidir:\n",
                "        train_df = create_bidirectional(train_df)\n",
                "        print(f\"After bidirectional: {len(train_df)}\")\n",
                "    else:\n",
                "        train_df['input_text'] = \"translate Akkadian to English: \" + train_df['transliteration'].astype(str)\n",
                "        train_df['target_text'] = train_df['translation'].astype(str)\n",
                "    \n",
                "    return train_df[['input_text', 'target_text']]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== Training Function ==========\n",
                "def train_model(config):\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Training: {config['name']}\")\n",
                "    print(f\"Epochs: {config['epochs']}, LR: {config['lr']}, MaxLen: {config['max_len']}\")\n",
                "    print(f\"{'='*60}\\n\")\n",
                "    \n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()\n",
                "    \n",
                "    # Load data\n",
                "    train_df = load_data(use_bidir=config['bidir'], use_extra=config['extra'])\n",
                "    dataset = Dataset.from_pandas(train_df)\n",
                "    split = dataset.train_test_split(test_size=0.1, seed=42)\n",
                "    \n",
                "    # Load model\n",
                "    tokenizer = AutoTokenizer.from_pretrained('google/byt5-small')\n",
                "    model = AutoModelForSeq2SeqLM.from_pretrained('google/byt5-small')\n",
                "    \n",
                "    # Tokenize\n",
                "    def preprocess(examples):\n",
                "        inputs = [str(x) for x in examples['input_text']]\n",
                "        targets = [str(x) for x in examples['target_text']]\n",
                "        model_inputs = tokenizer(inputs, max_length=config['max_len'], truncation=True)\n",
                "        labels = tokenizer(targets, max_length=config['max_len'], truncation=True)\n",
                "        model_inputs['labels'] = labels['input_ids']\n",
                "        return model_inputs\n",
                "    \n",
                "    train_tok = split['train'].map(preprocess, batched=True, remove_columns=split['train'].column_names)\n",
                "    val_tok = split['test'].map(preprocess, batched=True, remove_columns=split['test'].column_names)\n",
                "    \n",
                "    # Metrics\n",
                "    metric_chrf = evaluate.load('chrf')\n",
                "    metric_bleu = evaluate.load('sacrebleu')\n",
                "    \n",
                "    def compute_metrics(eval_preds):\n",
                "        preds, labels = eval_preds\n",
                "        if isinstance(preds, tuple): preds = preds[0]\n",
                "        if hasattr(preds, 'ndim') and preds.ndim == 3:\n",
                "            preds = np.argmax(preds, axis=-1)\n",
                "        preds = np.clip(preds.astype(np.int64), 0, tokenizer.vocab_size - 1)\n",
                "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
                "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
                "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
                "        chrf = metric_chrf.compute(predictions=decoded_preds, references=decoded_labels)['score']\n",
                "        bleu = metric_bleu.compute(predictions=decoded_preds, references=[[x] for x in decoded_labels])['score']\n",
                "        return {'chrf': chrf, 'bleu': bleu, 'geo_mean': (chrf * bleu) ** 0.5 if chrf > 0 and bleu > 0 else 0.0}\n",
                "    \n",
                "    # Training\n",
                "    output_dir = f\"./models/{config['name']}\"\n",
                "    args = Seq2SeqTrainingArguments(\n",
                "        output_dir=output_dir,\n",
                "        eval_strategy='epoch',\n",
                "        save_strategy='epoch',\n",
                "        learning_rate=config['lr'],\n",
                "        optim='adafactor',\n",
                "        label_smoothing_factor=config['label_smooth'],\n",
                "        fp16=False,\n",
                "        per_device_train_batch_size=1,\n",
                "        per_device_eval_batch_size=1,\n",
                "        gradient_accumulation_steps=8,\n",
                "        weight_decay=0.01,\n",
                "        save_total_limit=1,\n",
                "        num_train_epochs=config['epochs'],\n",
                "        predict_with_generate=True,\n",
                "        logging_steps=50,\n",
                "        report_to='none',\n",
                "        load_best_model_at_end=True,\n",
                "        metric_for_best_model='geo_mean',\n",
                "        greater_is_better=True,\n",
                "    )\n",
                "    \n",
                "    trainer = Seq2SeqTrainer(\n",
                "        model=model,\n",
                "        args=args,\n",
                "        train_dataset=train_tok,\n",
                "        eval_dataset=val_tok,\n",
                "        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
                "        tokenizer=tokenizer,\n",
                "        compute_metrics=compute_metrics,\n",
                "    )\n",
                "    \n",
                "    trainer.train()\n",
                "    trainer.save_model(output_dir)\n",
                "    tokenizer.save_pretrained(output_dir)\n",
                "    print(f\"Model saved to {output_dir}\")\n",
                "    \n",
                "    del model, trainer\n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== Train Selected Models ==========\n",
                "for model_id in MODEL_IDS:\n",
                "    train_model(CONFIGS[model_id])\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Training complete!\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save models as Kaggle dataset\n",
                "!ls -la ./models/"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":121150,"databundleVersionId":15061024,"sourceType":"competition"},{"sourceId":14661932,"sourceType":"datasetVersion","datasetId":9366631}],"dockerImageVersionId":31260,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1Ô∏è‚É£ System-Level Optimization\n\nSet optimal environment variables **before** importing PyTorch.","metadata":{}},{"cell_type":"code","source":"import os\n\n# Optimize PyTorch/CUDA performance\nos.environ['OMP_NUM_THREADS'] = '4'\nos.environ['MKL_NUM_THREADS'] = '4'\nos.environ['CUDA_LAUNCH_BLOCKING'] = '0'\nos.environ['TORCH_CUDNN_V8_API_ENABLED'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'true'\n\nprint(\"‚úÖ Environment variables optimized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:43:35.2211Z","iopub.execute_input":"2026-02-02T12:43:35.221621Z","iopub.status.idle":"2026-02-02T12:43:35.228523Z","shell.execute_reply.started":"2026-02-02T12:43:35.22157Z","shell.execute_reply":"2026-02-02T12:43:35.227203Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2Ô∏è‚É£ Imports & Setup","metadata":{}},{"cell_type":"code","source":"import re\nimport logging\nimport warnings\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Sampler\nfrom torch.cuda.amp import autocast\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom tqdm.auto import tqdm\nimport json\nimport random\n\nwarnings.filterwarnings('ignore')\n\n# Check GPU\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:43:35.230341Z","iopub.execute_input":"2026-02-02T12:43:35.230674Z","iopub.status.idle":"2026-02-02T12:43:35.255405Z","shell.execute_reply.started":"2026-02-02T12:43:35.230641Z","shell.execute_reply":"2026-02-02T12:43:35.253958Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3Ô∏è‚É£ Configuration\n\n**üìù EDIT THESE PATHS:**","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass UltraConfig:\n    \"\"\"Ultra-optimized configuration\"\"\"\n    \n    # ============ PATHS - EDIT THESE ============\n    test_data_path: str = \"/kaggle/input/deep-past-initiative-machine-translation/test.csv\"\n    model_path: str = \"/kaggle/input/final-byt5/byt5-akkadian-optimized-34x\"\n    output_dir: str = \"/kaggle/working/\"\n    \n    # ============ PROCESSING ============\n    max_length: int = 512\n    batch_size: int = 8  # Will auto-tune if use_auto_batch_size=True\n    num_workers: int = 4  # Increased for better throughput\n    \n    # ============ GENERATION ============\n    num_beams: int = 8\n    max_new_tokens: int = 512\n    length_penalty: float = 1.5\n    repetition_penalty: float = 1.2\n    early_stopping: bool = True\n    no_repeat_ngram_size: int = 0  # Set to 3 if you see repetition\n    \n    # ============ OPTIMIZATIONS ============\n    use_mixed_precision: bool = True      # FP16 for 2x speedup\n    use_better_transformer: bool = True   # 20-50% speedup\n    use_bucket_batching: bool = True      # 20-40% less padding\n    use_vectorized_postproc: bool = True  # 3-5x faster postproc\n    use_adaptive_beams: bool = True       # Smart beam allocation\n    use_auto_batch_size: bool = False     # Auto-find optimal batch size\n    \n    # ============ OTHER ============\n    aggressive_postprocessing: bool = True\n    checkpoint_freq: int = 100\n    num_buckets: int = 4  # For bucket batching\n    \n    def __post_init__(self):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        Path(self.output_dir).mkdir(exist_ok=True, parents=True)\n        \n        if not torch.cuda.is_available():\n            self.use_mixed_precision = False\n            self.use_better_transformer = False\n\n# Create config\nconfig = UltraConfig()\n\nprint(\"\\nüìã Configuration:\")\nprint(f\"  Device: {config.device}\")\nprint(f\"  Batch size: {config.batch_size}\")\nprint(f\"  Beams: {config.num_beams}\")\nprint(f\"\\nüöÄ Optimizations:\")\nprint(f\"  Mixed Precision: {config.use_mixed_precision}\")\nprint(f\"  BetterTransformer: {config.use_better_transformer}\")\nprint(f\"  Bucket Batching: {config.use_bucket_batching}\")\nprint(f\"  Vectorized Postproc: {config.use_vectorized_postproc}\")\nprint(f\"  Adaptive Beams: {config.use_adaptive_beams}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:43:35.257077Z","iopub.execute_input":"2026-02-02T12:43:35.257653Z","iopub.status.idle":"2026-02-02T12:43:35.277792Z","shell.execute_reply.started":"2026-02-02T12:43:35.257611Z","shell.execute_reply":"2026-02-02T12:43:35.276851Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4Ô∏è‚É£ Logging Setup","metadata":{}},{"cell_type":"code","source":"def setup_logging(output_dir: str = './outputs'):\n    \"\"\"Setup logging\"\"\"\n    Path(output_dir).mkdir(exist_ok=True, parents=True)\n    log_file = Path(output_dir) / 'inference_ultra.log'\n    \n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.StreamHandler(),\n            logging.FileHandler(log_file)\n        ]\n    )\n    return logging.getLogger(__name__)\n\nlogger = setup_logging(config.output_dir)\nlogger.info(\"Logging initialized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:43:35.279652Z","iopub.execute_input":"2026-02-02T12:43:35.279963Z","iopub.status.idle":"2026-02-02T12:43:35.309695Z","shell.execute_reply.started":"2026-02-02T12:43:35.279934Z","shell.execute_reply":"2026-02-02T12:43:35.308551Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5Ô∏è‚É£ Optimized Text Preprocessor\n\nUses pre-compiled regex patterns for speed.","metadata":{}},{"cell_type":"code","source":"class OptimizedPreprocessor:\n    \"\"\"Preprocessor with pre-compiled patterns\"\"\"\n    \n    def __init__(self):\n        # Pre-compile regex patterns (20-30% faster)\n        self.patterns = {\n            'big_gap': re.compile(r'(\\.{3,}|‚Ä¶+|‚Ä¶‚Ä¶)'),\n            'small_gap': re.compile(r'(xx+|\\s+x\\s+)'),\n        }\n    \n    def preprocess_input_text(self, text: str) -> str:\n        \"\"\"Single text preprocessing\"\"\"\n        if pd.isna(text):\n            return \"\"\n        \n        text = str(text)\n        text = self.patterns['big_gap'].sub('<big_gap>', text)\n        text = self.patterns['small_gap'].sub('<gap>', text)\n        \n        return text\n    \n    def preprocess_batch(self, texts: List[str]) -> List[str]:\n        \"\"\"Vectorized batch preprocessing (faster)\"\"\"\n        s = pd.Series(texts).fillna(\"\")\n        s = s.astype(str)\n        s = s.str.replace(self.patterns['big_gap'], '<big_gap>', regex=True)\n        s = s.str.replace(self.patterns['small_gap'], '<gap>', regex=True)\n        return s.tolist()\n\n# Test\npreprocessor = OptimizedPreprocessor()\ntest = \"lugal ... xxx mu.2.kam\"\nprint(f\"Test input:  {test}\")\nprint(f\"Preprocessed: {preprocessor.preprocess_input_text(test)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:43:35.311088Z","iopub.execute_input":"2026-02-02T12:43:35.311417Z","iopub.status.idle":"2026-02-02T12:43:35.329271Z","shell.execute_reply.started":"2026-02-02T12:43:35.311391Z","shell.execute_reply":"2026-02-02T12:43:35.328384Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6Ô∏è‚É£ Vectorized Postprocessor\n\nUses pandas for batch operations ‚Üí **3-5x faster** than loop-based postprocessing.","metadata":{}},{"cell_type":"code","source":"class VectorizedPostprocessor:\n    \"\"\"Ultra-fast vectorized postprocessing\"\"\"\n    \n    def __init__(self, aggressive: bool = True):\n        self.aggressive = aggressive\n        \n        # Pre-compile ALL patterns\n        self.patterns = {\n            'gap': re.compile(r'(\\[x\\]|\\(x\\)|\\bx\\b)', re.I),\n            'big_gap': re.compile(r'(\\.{3,}|‚Ä¶|\\[\\.+\\])'),\n            'annotations': re.compile(r'\\((fem|plur|pl|sing|singular|plural|\\?|!)\\..\\s*\\w*\\)', re.I),\n            'repeated_words': re.compile(r'\\b(\\w+)(?:\\s+\\1\\b)+'),\n            'whitespace': re.compile(r'\\s+'),\n            'punct_space': re.compile(r'\\s+([.,:])'),\n            'repeated_punct': re.compile(r'([.,])\\1+'),\n        }\n        \n        # Character translation tables\n        self.subscript_trans = str.maketrans(\"‚ÇÄ‚ÇÅ‚ÇÇ‚ÇÉ‚ÇÑ‚ÇÖ‚ÇÜ‚Çá‚Çà‚Çâ\", \"0123456789\")\n        self.special_chars_trans = str.maketrans('·∏´·∏™', 'hH')\n        self.forbidden_chars = '!?()\"‚Äî‚Äî<>‚åà‚åã‚åä[]+ æ/;'\n        self.forbidden_trans = str.maketrans('', '', self.forbidden_chars)\n    \n    def postprocess_batch(self, translations: List[str]) -> List[str]:\n        \"\"\"Vectorized batch postprocessing - 3-5x faster than loop\"\"\"\n        \n        # Convert to Series for vectorized operations\n        s = pd.Series(translations)\n        \n        # Filter invalid entries\n        valid_mask = s.apply(lambda x: isinstance(x, str) and x.strip())\n        if not valid_mask.all():\n            s[~valid_mask] = \"\"\n        \n        # Basic cleaning (always applied)\n        s = s.str.translate(self.special_chars_trans)\n        s = s.str.translate(self.subscript_trans)\n        s = s.str.replace(self.patterns['whitespace'], ' ', regex=True)\n        s = s.str.strip()\n        \n        if self.aggressive:\n            # Normalize gaps\n            s = s.str.replace(self.patterns['gap'], '<gap>', regex=True)\n            s = s.str.replace(self.patterns['big_gap'], '<big_gap>', regex=True)\n            \n            # Merge adjacent gaps\n            s = s.str.replace('<gap> <gap>', '<big_gap>', regex=False)\n            s = s.str.replace('<big_gap> <big_gap>', '<big_gap>', regex=False)\n            \n            # Remove annotations\n            s = s.str.replace(self.patterns['annotations'], '', regex=True)\n            \n            # Protect gaps during char removal\n            s = s.str.replace('<gap>', '\\x00GAP\\x00', regex=False)\n            s = s.str.replace('<big_gap>', '\\x00BIG\\x00', regex=False)\n            \n            # Remove forbidden characters\n            s = s.str.translate(self.forbidden_trans)\n            \n            # Restore gaps\n            s = s.str.replace('\\x00GAP\\x00', ' <gap> ', regex=False)\n            s = s.str.replace('\\x00BIG\\x00', ' <big_gap> ', regex=False)\n            \n            # Fractions (vectorized)\n            s = s.str.replace(r'(\\d+)\\.5\\b', r'\\1¬Ω', regex=True)\n            s = s.str.replace(r'\\b0\\.5\\b', '¬Ω', regex=True)\n            s = s.str.replace(r'(\\d+)\\.25\\b', r'\\1¬º', regex=True)\n            s = s.str.replace(r'\\b0\\.25\\b', '¬º', regex=True)\n            s = s.str.replace(r'(\\d+)\\.75\\b', r'\\1¬æ', regex=True)\n            s = s.str.replace(r'\\b0\\.75\\b', '¬æ', regex=True)\n            \n            # Remove repeated words\n            s = s.str.replace(self.patterns['repeated_words'], r'\\1', regex=True)\n            \n            # Remove repeated n-grams\n            for n in range(4, 1, -1):\n                pattern = r'\\b((?:\\w+\\s+){' + str(n-1) + r'}\\w+)(?:\\s+\\1\\b)+'\n                s = s.str.replace(pattern, r'\\1', regex=True)\n            \n            # Fix punctuation\n            s = s.str.replace(self.patterns['punct_space'], r'\\1', regex=True)\n            s = s.str.replace(self.patterns['repeated_punct'], r'\\1', regex=True)\n            \n            # Final cleanup\n            s = s.str.replace(self.patterns['whitespace'], ' ', regex=True)\n            s = s.str.strip().str.strip('-').str.strip()\n        \n        return s.tolist()\n\n# Test\npostprocessor = VectorizedPostprocessor(aggressive=config.aggressive_postprocessing)\ntest_outputs = [\n    \"The king (plur.) took the city... [x] [x]\",\n    \"He spoke spoke to the assembly\"\n]\ncleaned = postprocessor.postprocess_batch(test_outputs)\nprint(\"Test postprocessing:\")\nfor orig, clean in zip(test_outputs, cleaned):\n    print(f\"  {orig}\")\n    print(f\"  ‚Üí {clean}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:43:35.330573Z","iopub.execute_input":"2026-02-02T12:43:35.331178Z","iopub.status.idle":"2026-02-02T12:43:35.365688Z","shell.execute_reply.started":"2026-02-02T12:43:35.331137Z","shell.execute_reply":"2026-02-02T12:43:35.364841Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7Ô∏è‚É£ Bucket Batch Sampler\n\nGroups samples by length to minimize padding ‚Üí **20-40% faster**.","metadata":{}},{"cell_type":"code","source":"class BucketBatchSampler(Sampler):\n    \"\"\"Batch samples by similar length to minimize padding\"\"\"\n    \n    def __init__(self, dataset, batch_size: int, num_buckets: int = 4, shuffle: bool = False):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n        # Calculate lengths\n        lengths = [len(text.split()) for _, text in dataset]\n        \n        # Sort indices by length\n        sorted_indices = sorted(range(len(lengths)), key=lambda i: lengths[i])\n        \n        # Create buckets\n        bucket_size = len(sorted_indices) // num_buckets\n        self.buckets = []\n        for i in range(num_buckets):\n            start = i * bucket_size\n            end = None if i == num_buckets - 1 else (i + 1) * bucket_size\n            self.buckets.append(sorted_indices[start:end])\n        \n        # Log bucket info\n        logger.info(f\"Created {num_buckets} buckets:\")\n        for i, bucket in enumerate(self.buckets):\n            bucket_lengths = [lengths[idx] for idx in bucket]\n            logger.info(f\"  Bucket {i}: {len(bucket)} samples, \"\n                       f\"length range [{min(bucket_lengths)}, {max(bucket_lengths)}]\")\n    \n    def __iter__(self):\n        for bucket in self.buckets:\n            if self.shuffle:\n                random.shuffle(bucket)\n            \n            for i in range(0, len(bucket), self.batch_size):\n                yield bucket[i:i+self.batch_size]\n    \n    def __len__(self):\n        return sum((len(b) + self.batch_size - 1) // self.batch_size for b in self.buckets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:43:35.366998Z","iopub.execute_input":"2026-02-02T12:43:35.367356Z","iopub.status.idle":"2026-02-02T12:43:35.390225Z","shell.execute_reply.started":"2026-02-02T12:43:35.367321Z","shell.execute_reply":"2026-02-02T12:43:35.389024Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8Ô∏è‚É£ Dataset Class","metadata":{}},{"cell_type":"code","source":"class AkkadianDataset(Dataset):\n    \"\"\"Optimized dataset with batch preprocessing\"\"\"\n    \n    def __init__(self, dataframe: pd.DataFrame, preprocessor: OptimizedPreprocessor):\n        self.sample_ids = dataframe['id'].tolist()\n        \n        # Batch preprocess (faster than loop)\n        raw_texts = dataframe['transliteration'].tolist()\n        preprocessed = preprocessor.preprocess_batch(raw_texts)\n        \n        # Add task prefix\n        self.input_texts = [\n            \"translate Akkadian to English: \" + text\n            for text in preprocessed\n        ]\n        \n        logger.info(f\"Dataset created with {len(self.sample_ids)} samples\")\n    \n    def __len__(self):\n        return len(self.sample_ids)\n    \n    def __getitem__(self, index: int):\n        return self.sample_ids[index], self.input_texts[index]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:43:35.391685Z","iopub.execute_input":"2026-02-02T12:43:35.392034Z","iopub.status.idle":"2026-02-02T12:43:35.418247Z","shell.execute_reply.started":"2026-02-02T12:43:35.392006Z","shell.execute_reply":"2026-02-02T12:43:35.416926Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9Ô∏è‚É£ Ultra-Optimized Inference Engine\n\nMain inference engine with all optimizations.","metadata":{}},{"cell_type":"code","source":"class UltraInferenceEngine:\n    \"\"\"Ultra-optimized inference engine\"\"\"\n    \n    def __init__(self, config: UltraConfig):\n        self.config = config\n        self.preprocessor = OptimizedPreprocessor()\n        self.postprocessor = VectorizedPostprocessor(aggressive=config.aggressive_postprocessing)\n        self.results = []\n        \n        # Load model\n        self._load_model()\n    \n    def _load_model(self):\n        \"\"\"Load and optimize model\"\"\"\n        logger.info(f\"Loading model from {self.config.model_path}\")\n        \n        try:\n            self.model = AutoModelForSeq2SeqLM.from_pretrained(\n                self.config.model_path\n            ).to(self.config.device).eval()\n            \n            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path)\n            \n            num_params = sum(p.numel() for p in self.model.parameters())\n            logger.info(f\"Model loaded: {num_params:,} parameters\")\n            \n            # Apply BetterTransformer\n            if self.config.use_better_transformer and torch.cuda.is_available():\n                try:\n                    from optimum.bettertransformer import BetterTransformer\n                    logger.info(\"Applying BetterTransformer...\")\n                    self.model = BetterTransformer.transform(self.model)\n                    logger.info(\"‚úÖ BetterTransformer applied (20-50% speedup)\")\n                except ImportError:\n                    logger.warning(\"‚ö†Ô∏è  'optimum' not installed, skipping BetterTransformer\")\n                    logger.warning(\"   Install with: !pip install optimum\")\n                except Exception as e:\n                    logger.warning(f\"‚ö†Ô∏è  BetterTransformer failed: {e}\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to load model: {e}\")\n            raise\n    \n    def _collate_fn(self, batch_samples):\n        \"\"\"Collate function\"\"\"\n        batch_ids = [s[0] for s in batch_samples]\n        batch_texts = [s[1] for s in batch_samples]\n        \n        tokenized = self.tokenizer(\n            batch_texts,\n            max_length=self.config.max_length,\n            padding=True,\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        return batch_ids, tokenized\n    \n    def find_optimal_batch_size(self, dataset, start_bs: int = 32):\n        \"\"\"Binary search for optimal batch size\"\"\"\n        logger.info(\"üîç Finding optimal batch size...\")\n        \n        max_bs = start_bs\n        min_bs = 1\n        \n        while max_bs - min_bs > 1:\n            test_bs = (max_bs + min_bs) // 2\n            \n            try:\n                test_batch = [dataset[i] for i in range(min(test_bs, len(dataset)))]\n                ids, inputs = self._collate_fn(test_batch)\n                \n                with torch.inference_mode():\n                    if self.config.use_mixed_precision:\n                        with autocast():\n                            outputs = self.model.generate(\n                                input_ids=inputs.input_ids.to(self.config.device),\n                                attention_mask=inputs.attention_mask.to(self.config.device),\n                                num_beams=self.config.num_beams,\n                                max_new_tokens=64,\n                                use_cache=True\n                            )\n                    else:\n                        outputs = self.model.generate(\n                            input_ids=inputs.input_ids.to(self.config.device),\n                            attention_mask=inputs.attention_mask.to(self.config.device),\n                            num_beams=self.config.num_beams,\n                            max_new_tokens=64,\n                            use_cache=True\n                        )\n                \n                min_bs = test_bs\n                logger.info(f\"  ‚úÖ Batch size {test_bs} works\")\n                \n                del outputs, inputs\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                \n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    max_bs = test_bs\n                    logger.info(f\"  ‚ùå Batch size {test_bs} OOM\")\n                    if torch.cuda.is_available():\n                        torch.cuda.empty_cache()\n                else:\n                    raise\n        \n        optimal = min_bs\n        logger.info(f\"üéØ Optimal batch size: {optimal}\")\n        return optimal\n    \n    def _get_adaptive_beam_size(self, input_ids, attention_mask):\n        \"\"\"Adaptive beam size based on complexity\"\"\"\n        if not self.config.use_adaptive_beams:\n            return self.config.num_beams\n        \n        lengths = attention_mask.sum(dim=1)\n        \n        # Short ‚Üí fewer beams, Long ‚Üí more beams\n        beam_sizes = torch.where(\n            lengths < 100,\n            torch.tensor(max(4, self.config.num_beams // 2)),\n            torch.tensor(self.config.num_beams)\n        )\n        \n        return beam_sizes[0].item()\n    \n    def _save_checkpoint(self):\n        \"\"\"Save checkpoint\"\"\"\n        if len(self.results) > 0 and len(self.results) % self.config.checkpoint_freq == 0:\n            path = Path(self.config.output_dir) / f\"checkpoint_{len(self.results)}.csv\"\n            df = pd.DataFrame(self.results, columns=['id', 'translation'])\n            df.to_csv(path, index=False)\n            logger.info(f\"üíæ Checkpoint: {len(self.results)} translations\")\n    \n    def run_inference(self, test_df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Run ultra-optimized inference\"\"\"\n        logger.info(\"üöÄ Starting ULTRA-OPTIMIZED inference\")\n        \n        # Create dataset\n        dataset = AkkadianDataset(test_df, self.preprocessor)\n        \n        # Auto-find batch size\n        if self.config.use_auto_batch_size:\n            optimal_bs = self.find_optimal_batch_size(dataset)\n            self.config.batch_size = optimal_bs\n        \n        # Create dataloader\n        if self.config.use_bucket_batching:\n            batch_sampler = BucketBatchSampler(\n                dataset, \n                self.config.batch_size,\n                num_buckets=self.config.num_buckets\n            )\n            dataloader = DataLoader(\n                dataset,\n                batch_sampler=batch_sampler,\n                num_workers=self.config.num_workers,\n                collate_fn=self._collate_fn,\n                pin_memory=True,\n                prefetch_factor=2,\n                persistent_workers=True if self.config.num_workers > 0 else False\n            )\n        else:\n            dataloader = DataLoader(\n                dataset,\n                batch_size=self.config.batch_size,\n                shuffle=False,\n                num_workers=self.config.num_workers,\n                collate_fn=self._collate_fn,\n                pin_memory=True,\n                prefetch_factor=2,\n                persistent_workers=True if self.config.num_workers > 0 else False\n            )\n        \n        logger.info(f\"DataLoader created: {len(dataloader)} batches\")\n        logger.info(f\"Active optimizations:\")\n        logger.info(f\"  ‚úÖ Mixed Precision: {self.config.use_mixed_precision}\")\n        logger.info(f\"  ‚úÖ BetterTransformer: {self.config.use_better_transformer}\")\n        logger.info(f\"  ‚úÖ Bucket Batching: {self.config.use_bucket_batching}\")\n        logger.info(f\"  ‚úÖ Vectorized Postproc: {self.config.use_vectorized_postproc}\")\n        logger.info(f\"  ‚úÖ Adaptive Beams: {self.config.use_adaptive_beams}\")\n        \n        # Generation config\n        # Generation config\n        base_gen_config = {\n            \"max_new_tokens\": self.config.max_new_tokens,\n            \"length_penalty\": self.config.length_penalty,\n            \"repetition_penalty\": self.config.repetition_penalty,  # ADD THIS LINE\n            \"early_stopping\": self.config.early_stopping,\n            \"use_cache\": True,\n        }\n        if self.config.no_repeat_ngram_size > 0:\n            base_gen_config[\"no_repeat_ngram_size\"] = self.config.no_repeat_ngram_size\n        \n        # Run inference\n        self.results = []\n        \n        with torch.inference_mode():\n            for batch_idx, (batch_ids, tokenized) in enumerate(tqdm(dataloader, desc=\"üöÄ Translating\")):\n                try:\n                    input_ids = tokenized.input_ids.to(self.config.device)\n                    attention_mask = tokenized.attention_mask.to(self.config.device)\n                    \n                    # Adaptive beam size\n                    beam_size = self._get_adaptive_beam_size(input_ids, attention_mask)\n                    gen_config = {**base_gen_config, \"num_beams\": beam_size}\n                    \n                    # Generate\n                    if self.config.use_mixed_precision:\n                        with autocast():\n                            outputs = self.model.generate(\n                                input_ids=input_ids,\n                                attention_mask=attention_mask,\n                                **gen_config\n                            )\n                    else:\n                        outputs = self.model.generate(\n                            input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            **gen_config\n                        )\n                    \n                    # Decode\n                    translations = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n                    \n                    # Postprocess (vectorized)\n                    if self.config.use_vectorized_postproc:\n                        cleaned = self.postprocessor.postprocess_batch(translations)\n                    else:\n                        # Fallback to single processing\n                        cleaned = [self.postprocessor.postprocess_batch([t])[0] for t in translations]\n                    \n                    # Store\n                    self.results.extend(zip(batch_ids, cleaned))\n                    \n                    # Checkpoint\n                    self._save_checkpoint()\n                    \n                    # Memory cleanup\n                    if torch.cuda.is_available() and batch_idx % 10 == 0:\n                        torch.cuda.empty_cache()\n                    \n                except Exception as e:\n                    logger.error(f\"‚ùå Batch {batch_idx} error: {e}\")\n                    self.results.extend([(bid, \"\") for bid in batch_ids])\n                    continue\n        \n        logger.info(\"‚úÖ Inference completed\")\n        \n        # Create results\n        results_df = pd.DataFrame(self.results, columns=['id', 'translation'])\n        self._validate_results(results_df)\n        \n        return results_df\n    \n    def _validate_results(self, df: pd.DataFrame):\n        \"\"\"Validation report\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"üìä VALIDATION REPORT\")\n        print(\"=\"*60)\n        \n        empty = df['translation'].str.strip().eq('').sum()\n        print(f\"\\nEmpty: {empty} ({empty/len(df)*100:.2f}%)\")\n        \n        lengths = df['translation'].str.len()\n        print(f\"\\nüìè Length stats:\")\n        print(f\"   Mean: {lengths.mean():.1f}, Median: {lengths.median():.1f}\")\n        print(f\"   Min: {lengths.min()}, Max: {lengths.max()}\")\n        \n        short = ((lengths < 5) & (lengths > 0)).sum()\n        if short > 0:\n            print(f\"   ‚ö†Ô∏è  {short} very short translations\")\n        \n        print(f\"\\nüìù Sample translations:\")\n        for idx in [0, len(df)//2, -1]:\n            s = df.iloc[idx]\n            preview = s['translation'][:70] + \"...\" if len(s['translation']) > 70 else s['translation']\n            print(f\"   ID {s['id']:4d}: {preview}\")\n        \n        print(\"\\n\" + \"=\"*60 + \"\\n\")\n\nprint(\"‚úÖ Inference engine defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:43:35.421032Z","iopub.execute_input":"2026-02-02T12:43:35.421419Z","iopub.status.idle":"2026-02-02T12:43:35.463436Z","shell.execute_reply.started":"2026-02-02T12:43:35.42139Z","shell.execute_reply":"2026-02-02T12:43:35.461998Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üîü Load Test Data","metadata":{}},{"cell_type":"code","source":"logger.info(f\"Loading test data from {config.test_data_path}\")\n\ntest_df = pd.read_csv(config.test_data_path, encoding='utf-8')\nlogger.info(f\"‚úÖ Loaded {len(test_df)} test samples\")\n\nprint(\"\\nFirst 5 samples:\")\nprint(test_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:43:35.466378Z","iopub.execute_input":"2026-02-02T12:43:35.466771Z","iopub.status.idle":"2026-02-02T12:43:35.502786Z","shell.execute_reply.started":"2026-02-02T12:43:35.466742Z","shell.execute_reply":"2026-02-02T12:43:35.50142Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1Ô∏è‚É£1Ô∏è‚É£ Run Ultra-Optimized Inference\n\n**This is the main cell - all optimizations are active!**","metadata":{}},{"cell_type":"code","source":"# Create engine\nengine = UltraInferenceEngine(config)\n\n# Run inference\nresults_df = engine.run_inference(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:43:35.504494Z","iopub.execute_input":"2026-02-02T12:43:35.504925Z","iopub.status.idle":"2026-02-02T12:45:02.045487Z","shell.execute_reply.started":"2026-02-02T12:43:35.504869Z","shell.execute_reply":"2026-02-02T12:45:02.044273Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1Ô∏è‚É£2Ô∏è‚É£ Save Results","metadata":{}},{"cell_type":"code","source":"# Save submission\noutput_path = Path(config.output_dir) / 'submission.csv'\nresults_df.to_csv(output_path, index=False)\nlogger.info(f\"\\n‚úÖ Submission saved to {output_path}\")\n\n# Save config\nconfig_dict = {\n    \"batch_size\": config.batch_size,\n    \"num_beams\": config.num_beams,\n    \"length_penalty\": config.length_penalty,\n    \"no_repeat_ngram_size\": config.no_repeat_ngram_size,\n    \"optimizations\": {\n        \"mixed_precision\": config.use_mixed_precision,\n        \"better_transformer\": config.use_better_transformer,\n        \"bucket_batching\": config.use_bucket_batching,\n        \"vectorized_postproc\": config.use_vectorized_postproc,\n        \"adaptive_beams\": config.use_adaptive_beams,\n    }\n}\n\nconfig_path = Path(config.output_dir) / 'ultra_config.json'\nwith open(config_path, 'w') as f:\n    json.dump(config_dict, f, indent=2)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üéâ ULTRA-OPTIMIZED INFERENCE COMPLETE!\")\nprint(\"=\"*60)\nprint(f\"Submission file: {output_path}\")\nprint(f\"Config file: {config_path}\")\nprint(f\"Log file: {Path(config.output_dir) / 'inference_ultra.log'}\")\nprint(f\"Total translations: {len(results_df)}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:45:02.047768Z","iopub.execute_input":"2026-02-02T12:45:02.04816Z","iopub.status.idle":"2026-02-02T12:45:02.064826Z","shell.execute_reply.started":"2026-02-02T12:45:02.048112Z","shell.execute_reply":"2026-02-02T12:45:02.0639Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1Ô∏è‚É£3Ô∏è‚É£ [Optional] Inspect Results","metadata":{}},{"cell_type":"code","source":"# Load submission\nsubmission = pd.read_csv(output_path)\n\nprint(f\"Submission shape: {submission.shape}\")\nprint(f\"\\nFirst 10 translations:\")\nprint(submission.head(10))\n\nprint(f\"\\nLast 10 translations:\")\nprint(submission.tail(10))\n\n# Statistics\nlengths = submission['translation'].str.len()\nprint(f\"\\nLength distribution:\")\nprint(lengths.describe())\n\n# Check for issues\nempty = submission['translation'].str.strip().eq('').sum()\nprint(f\"\\nEmpty translations: {empty}\")\n\nif empty > 0:\n    print(\"\\nEmpty translation IDs:\")\n    print(submission[submission['translation'].str.strip().eq('')]['id'].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-02T12:45:02.066028Z","iopub.execute_input":"2026-02-02T12:45:02.066371Z","iopub.status.idle":"2026-02-02T12:45:02.096855Z","shell.execute_reply.started":"2026-02-02T12:45:02.066339Z","shell.execute_reply":"2026-02-02T12:45:02.095834Z"}},"outputs":[],"execution_count":null}]}